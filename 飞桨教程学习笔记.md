# 作者说明

该笔记更加注重实操时候的记录，所以会舍去一部分内容

每一个小_大章节都会有一个链接连接到该章节最全的代码

# 第一章：零基础入门深度学习

## 1. 前言

## 2. 机器学习和深度学习综述

概括来说，人工智能、机器学习和深度学习覆盖的技术范畴是逐层递减的。人工智能是最宽泛的概念。机器学习是当前比较有效的一种实现人工智能的方式。深度学习是机器学习算法中最热门的一个分支，近些年取得了显著的进展，并替代了大多数传统机器学习算法。

<img src="https://ai-studio-static-online.cdn.bcebos.com/5521d1d951c440eb8511f03a0b9028bd63357aec52e94189b5ab3f55d63369d7" alt="img" style="zoom: 50%;" />



### 2.1 机器学习

机器学习是专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构，使之不断改善自身的性能。



## 3. 使用python语言和Numpy库来构建神经网络

**[最终代码形态](#3.1)**

### 3.1 线性回归模型

注：该例子以预测波士顿房价为例

![image-20211216194835609](飞桨教程学习笔记.assets/image-20211216194835609.png)

### 3.2 线性回归模型的神经网络结构

![image-20211216194930496](飞桨教程学习笔记.assets/image-20211216194930496-16396553740982.png)

### 3.3 构建波士顿房价预测任务的神经网络模型

#### 3.3.1 数据处理

**数据处理包含五个部分**：数据导入、数据形状变换、数据集划分、数据归一化处理和封装`load data`函数。数据预处理后，才能被模型调用。

------

- **读入数据**：通过如下代码读入数据，了解下波士顿房价的数据集结构，数据存放在本地目录下housing.data文件中。

- **数据形状变换**：由于读入的原始数据是1维的，所有数据都连在一起。因此需要我们将数据的形状进行变换，形成一个2维的矩阵，每行为一个数据样本（14个值），每个数据样本包含13个**_X_**（影响房价的特征）和一个_**Y**_（该类型房屋的均价）。

- **数据集划分**：将数据集划分成训练集和测试集，其中训练集用于确定模型的参数，测试集用于评判模型的效果。

- **数据归一化处理**：对每个特征进行归一化处理，使得每个特征的取值缩放到0~1之间。这样做有两个好处：一是模型训练更高效；二是特征前的权重大小可以代表该变量对预测结果的贡献度（因为每个特征值本身的范围相同）。

- **封装成load data函数**：将上述几个数据处理操作封装成`load data`函数，以便下一步模型的调用，实现方法如下。

  ```python
  def load_data():
      # 从文件导入数据
      datafile = './work/housing.data'
      data = np.fromfile(datafile, sep=' ')
  
      # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数
      feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \
                        'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]
      feature_num = len(feature_names)
  
      # 将原始数据进行Reshape，变成[N, 14]这样的形状
      data = data.reshape([data.shape[0] // feature_num, feature_num])
  
      # 将原数据集拆分成训练集和测试集
      # 这里使用80%的数据做训练，20%的数据做测试
      # 测试集和训练集必须是没有交集的
      ratio = 0.8
      offset = int(data.shape[0] * ratio)
      training_data = data[:offset]
  
      # 计算训练集的最大值，最小值，平均值
      maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \
                                   training_data.sum(axis=0) / training_data.shape[0]
  
      # 对数据进行归一化处理
      for i in range(feature_num):
          #print(maximums[i], minimums[i], avgs[i])
          data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])
  
      # 训练集和测试集的划分比例
      training_data = data[:offset]  
      test_data = data[offset:]
      return training_data, test_data
  
  
  #调用load_data()函数，检验
  # 获取数据
  training_data, test_data = load_data()
  x = training_data[:, :-1]    #横全取  纵除最后一个都取
  y = training_data[:, -1:]    #横全取  纵取最后一个
  # 查看数据
  print(x[0])
  print(y[0])
  ```

  

#### 3.3.2 模型设计

模型设计是深度学习模型关键要素之一，也称为网络结构设计，相当于模型的假设空间，即实现模型“前向计算”（从输入到输出）的过程。

![image-20211216201201506](飞桨教程学习笔记.assets/image-20211216201201506.png)

```python
w = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0] #定义权重
w = np.array(w).reshape([13, 1])                   #将其改成为13列 1行
```

取出第1条样本数据，观察样本的特征向量与参数向量相乘的结果。

```python
x1=x[0]
t = np.dot(x1, w)     #矩阵的乘法   取第一行的数据，乘以所有的权重，得出结果
print(t)
```

完整的线性回归公式，还需要初始化偏移量_b_，同样随意赋初值-0.2。那么，线性回归模型的完整输出是**_z_ = _t_ + _b_**，这个从特征和参数计算输出值的过程称为“前向计算”。

```python
b = -0.2
z = t + b
print(z)
```

将上述计算预测输出的过程以“类和对象”的方式来描述，类成员变量有参数**_w_**和**_b_**。通过写一个`forward`函数（代表“前向计算”）完成上述从特征和参数到输出预测值的计算过程，代码如下所示。

```python
class Network(object):
    def __init__(self, num_of_weights):
        # 随机产生w的初始值
        # 为了保持程序每次运行结果的一致性，
        # 此处设置固定的随机数种子
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)  #每个维度的个数，这里有两个维度 生成的是矩阵
        self.b = 0.                                  #指定b为0
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b               #获取预测值z
        return z
```

基于Network类的定义，模型的计算过程如下所示。

```python
net = Network(13)                                    #设置神经网络有13个特征参数，就会执行上面第一个函数，num_of_weights应该对应的就是13
x1 = x[0]                                            #获取第一个样例
y1 = y[0]                                            #获取第一个样例的真实值
z = net.forward(x1)                                  #执行向前传播函数，计算出z值
print(z)
```



#### 3.3.3 训练配置

模型设计完成后，需要通过训练配置寻找模型的最优值，即通过损失函数来衡量模型的好坏。训练配置也是深度学习模型关键要素之一。

![image-20211216202159305](飞桨教程学习笔记.assets/image-20211216202159305.png)

因为计算损失函数时需要把每个样本的损失函数值都考虑到，所以我们需要对单个样本的损失函数进行求和，并除以样本总数_**N**_。

![image-20211216202910236](飞桨教程学习笔记.assets/image-20211216202910236.png)

在Network类下面添加损失函数的计算过程如下：

```python
class Network(object):                                       #对类进行一个更新，添加了loss损失值的计算
    def __init__(self, num_of_weights):
        # 随机产生w的初始值
        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):                                    #这里在Network里计算损失值
        error = z - y                                        #这里参数定义的名字也就有意义了
        cost = error * error
        cost = np.mean(cost)                                 #对所有的值求均值
        return cost

```

使用定义的Network类，可以方便的计算预测值和损失函数。需要注意的是，类中的变量x, w，b, z, error等均是向量。以变量x为例，共有两个维度，一个代表特征数量（值为13），一个代表样本数量，代码如下所示。

```python
net = Network(13)                            #需要注意我们计算的都是向量
# 此处可以一次性计算多个样本的预测值和损失函数
x1 = x[0:3]                                  #取前三个样例的特征值
y1 = y[0:3]                                  #取前三个样例的真实值
z = net.forward(x1)                          #进行向前传播（注意是矩阵计算）
print('predict: ', z)                        #得出预测值
loss = net.loss(z, y1)                       #得出损失值
print('loss:', loss)                         #输出损失值
```



#### 3.3.4 训练过程

上述计算过程描述了如何构建神经网络，通过神经网络完成预测值和损失函数的计算。接下来介绍如何求解参数w和b的数值，这个过程也称为模型训练过程。训练过程是深度学习模型的关键要素之一，其目标是让定义的损失函数Loss尽可能的小，也就是说找到一个参数解w和b，使得损失函数取得极小值。

<img src="飞桨教程学习笔记.assets/image-20211216203608043.png" alt="image-20211216203608043" style="zoom:67%;" />

##### 梯度下降法

![image-20211216203706002](飞桨教程学习笔记.assets/image-20211216203706002.png)

 

```python
net = Network(13)
losses = []
#只画出参数w5和w9在区间[-160, 160]的曲线部分，以及包含损失函数的极值
w5 = np.arange(-160.0, 160.0, 1.0)                     #参数1：起点 参数2：终点 参数3：步长
w9 = np.arange(-160.0, 160.0, 1.0)
losses = np.zeros([len(w5), len(w9)])                  #设置矩阵，维度1为w5的长度，维度2位w9的长度

#计算设定区域内每个参数取值所对应的Loss
for i in range(len(w5)):
    for j in range(len(w9)):
        net.w[5] = w5[i]                               #设置对应的值
        net.w[9] = w9[j]
        z = net.forward(x)                             #进行向前传播
        loss = net.loss(z, y)                          #计算损失值
        losses[i, j] = loss                            #获得对应矩阵位置的值

#使用matplotlib将两个变量和对应的Loss作3D图
import matplotlib.pyplot as plt                        #获取类包
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()                                     #来一个图容器
ax = Axes3D(fig)                                       #变成3d版

w5, w9 = np.meshgrid(w5, w9)                           #生成网格坐标点矩阵

ax.plot_surface(w5, w9, losses, rstride=1, cstride=1, cmap='rainbow')  #x轴数据，y轴数据，z轴数据，指定行的跨度，指定列的跨度，设置颜色映射
plt.show()                                             #显示
```

输出的图片为下图：

<img src="飞桨教程学习笔记.assets/image-20211216203815166.png" alt="image-20211216203815166" style="zoom:67%;" />

![image-20211216204553105](飞桨教程学习笔记.assets/image-20211216204553105.png)

![image-20211216204653236](飞桨教程学习笔记.assets/image-20211216204653236.png)

![image-20211216204705598](飞桨教程学习笔记.assets/image-20211216204705598.png)

##### 计算梯度

![image-20211216204958572](飞桨教程学习笔记.assets/image-20211216204958572.png)

![image-20211216205013996](飞桨教程学习笔记.assets/image-20211216205013996.png)

可以通过具体的程序查看每个变量的数据和维度

```python
x1 = x[0]                                      #取第一个样例
y1 = y[0]                                      #取真实值
z1 = net.forward(x1)                           #向前传播，获取预测值
print('x1 {}, shape {}'.format(x1, x1.shape))  #输出特征值
print('y1 {}, shape {}'.format(y1, y1.shape))  #输出真实值
print('z1 {}, shape {}'.format(z1, z1.shape))  #输出预测值
```

按上面的公式，当只有一个样本时，可以计算某个w_j的梯度。比如w_0的梯度

```python
gradient_w0 = (z1 - y1) * x1[0]               #计算梯度
print('gradient_w0 {}'.format(gradient_w0))   #输出梯度值
```

##### 使用Numpy进行梯度计算

![image-20211216205544856](飞桨教程学习笔记.assets/image-20211216205544856.png)

输入数据中有多个样本，每个样本都对梯度有贡献。如上代码计算了只有样本1时的梯度值，同样的计算方法也可以计算样本2和样本3对梯度的贡献。

```python
gradient_w = (z1 - y1) * x1                  #直接计算整个矩阵的梯度值
print('gradient_w_by_sample1 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))  #输出梯度值

x2 = x[1]                                   #获取第二个样本特征值
y2 = y[1]                                   #获取第二个样本的真实值
z2 = net.forward(x2)                        #向前传播
gradient_w = (z2 - y2) * x2                 #计算梯度值
print('gradient_w_by_sample2 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))  #输出梯度值

#然后把1换成2就能够计算第三个赝本了
```

可能有的读者再次想到可以使用for循环把每个样本对梯度的贡献都计算出来，然后再作平均。但是我们不需要这么做，仍然可以使用Numpy的矩阵操作来简化运算，如3个样本的情况。

下面的x3samples, y3samples, z3samples的第一维大小均为3，表示有3个样本。下面计算这3个样本对梯度的贡献。

```python
# 注意这里是一次取出3个样本的数据，不是取出第3个样本
x3samples = x[0:3]                                 #和上面一样，这里一次性去出3个样本数据，这里都是矩阵运算
y3samples = y[0:3]
z3samples = net.forward(x3samples)

print('x {}, shape {}'.format(x3samples, x3samples.shape))
print('y {}, shape {}'.format(y3samples, y3samples.shape))
print('z {}, shape {}'.format(z3samples, z3samples.shape))

gradient_w = (z3samples - y3samples) * x3samples                               #计算梯度值
print('gradient_w {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))
```

接下来就是扩展到一般

![image-20211216210450687](飞桨教程学习笔记.assets/image-20211216210450687.png)

```python
z = net.forward(x)                                              #明白Numpy库的作用，这里是向前传播，计算所有的预测值
gradient_w = (z - y) * x                                        #计算所有的梯度值
print('gradient_w shape {}'.format(gradient_w.shape))           #共计404个样本，每个样本有13个值
print(gradient_w)
```

![image-20211216210552235](飞桨教程学习笔记.assets/image-20211216210552235.png)

```python
# axis = 0 表示把每一行做相加然后再除以总的行数
gradient_w = np.mean(gradient_w, axis=0)            #求均值
print('gradient_w ', gradient_w.shape)              #输出，有13个梯度值
print('w ', net.w.shape)                            #输出矩阵w的特征值
print(gradient_w)                                   #输出w的梯度值
print(net.w)                                        #输出特征参数
```

![image-20211216211052872](飞桨教程学习笔记.assets/image-20211216211052872.png)

```python
# axis = 0 表示把每一行做相加然后再除以总的行数
gradient_w = np.mean(gradient_w, axis=0)            #求均值
print('gradient_w ', gradient_w.shape)              #输出，有13个梯度值
print('w ', net.w.shape)                            #输出矩阵w的特征值
print(gradient_w)                                   #输出w的梯度值
print(net.w)                                        #输出特征参数

```

![image-20211216211244060](飞桨教程学习笔记.assets/image-20211216211244060.png)

```python
gradient_w = gradient_w[:, np.newaxis]        #np.newaxis 插入新维度，让gradient_w重回1维
print('gradient_w shape', gradient_w.shape)   #输出梯度值的维度
```

综上，计算梯度的代码如下所示。

```python
z = net.forward(x)                           #计算预测值
gradient_w = (z - y) * x                     #计算梯度值
gradient_w = np.mean(gradient_w, axis=0)     #计算平均梯度值的影响
gradient_w = gradient_w[:, np.newaxis]       #重新从0维变为1维
gradient_w                                   #输出梯度值
```

将上面计算w和b的梯度的过程，写成Network类的`gradient`函数，实现方法如下所示。

```python
class Network(object):                                     #添加了梯度下降的函数
    def __init__(self, num_of_weights):
        # 随机产生w的初始值
        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):                             #封装在这
        z = self.forward(x)                               #计算预测值
        gradient_w = (z-y)*x                              #计算所有梯度下降值
        gradient_w = np.mean(gradient_w, axis=0)          #取平均梯度值
        gradient_w = gradient_w[:, np.newaxis]            #重新从0维升到1维
        gradient_b = (z - y)                              #计算b的梯度值
        gradient_b = np.mean(gradient_b)                  #计算平均梯度值
        
        return gradient_w, gradient_b                     #返回梯度值w，b
    
    
# 调用上面定义的gradient函数，计算梯度
# 初始化网络
net = Network(13)                                        #设置有13个参数
# 设置[w5, w9] = [-100., -100.]
net.w[5] = -100.0                                        #还是使用两个特征参数，作为样例
net.w[9] = -100.0

z = net.forward(x)                                       #进行向前传播
loss = net.loss(z, y)                                    #计算损失值
gradient_w, gradient_b = net.gradient(x, y)              #计算梯度值
gradient_w5 = gradient_w[5][0]                           #赋值特征值
gradient_w9 = gradient_w[9][0]
print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))      #输出对应的点，和对应的损失值
print('gradient {}'.format([gradient_w5, gradient_w9]))  #输出对应的梯度值
```

##### 确定损失函数更小的点

下面我们开始研究更新梯度的方法。首先沿着梯度的反方向移动一小步，找到下一个点P1，观察损失函数的变化。

```python
# 在[w5, w9]平面上，沿着梯度的反方向移动到下一个点P1
# 定义移动步长 eta
eta = 0.1
# 更新参数w5和w9
net.w[5] = net.w[5] - eta * gradient_w5
net.w[9] = net.w[9] - eta * gradient_w9
# 重新计算z和loss
z = net.forward(x)                                 #这边的式子和上面一样理解
loss = net.loss(z, y)
gradient_w, gradient_b = net.gradient(x, y)
gradient_w5 = gradient_w[5][0]
gradient_w9 = gradient_w[9][0]
print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))
print('gradient {}'.format([gradient_w5, gradient_w9]))
```

![image-20211216211835179](飞桨教程学习笔记.assets/image-20211216211835179.png)

##### 代码封装

将上面的循环计算过程封装在`train`和`update`函数中，实现方法如下所示。

```python
class Network(object):    #新增一个函数
    def __init__(self, num_of_weights):
        # 随机产生w的初始值
        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights,1)
        self.w[5] = -100.
        self.w[9] = -100.
        self.b = 0.
        
    def forward(self, x):   #向前传播，计算预测值
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):   #计算损失值
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):   #计算梯度值
        z = self.forward(x)
        gradient_w = (z-y)*x
        gradient_w = np.mean(gradient_w, axis=0)  #计算平均梯度值
        gradient_w = gradient_w[:, np.newaxis]    #将0维升成1维
        gradient_b = (z - y)                      #计算梯度值b
        gradient_b = np.mean(gradient_b)          #其平均值
        return gradient_w, gradient_b
    
    def update(self, gradient_w5, gradient_w9, eta=0.01):  #使用梯度下降迭代更新值  eta是学习率
        net.w[5] = net.w[5] - eta * gradient_w5
        net.w[9] = net.w[9] - eta * gradient_w9
        
    def train(self, x, y, iterations=100, eta=0.01):    #分装train函数 iterations是迭代次数
        points = []                                     #定义两个矩阵，一个是记录对应点，一个是记录对应的loss损失值
        losses = []
        for i in range(iterations):                     #迭代一次
            points.append([net.w[5][0], net.w[9][0]])   #我们就举两个特征参数为例子
            z = self.forward(x)                         #向前传播，计算预测值
            L = self.loss(z, y)                         #计算损失值
            gradient_w, gradient_b = self.gradient(x, y)#计算梯度值
            gradient_w5 = gradient_w[5][0]              #进行赋值
            gradient_w9 = gradient_w[9][0]
            self.update(gradient_w5, gradient_w9, eta)  #更新值
            losses.append(L)                            #存储损失值（用于查看是不是慢慢递减）
            if i % 50 == 0:                             #满50就输出一次结果
                print('iter {}, point {}, loss {}'.format(i, [net.w[5][0], net.w[9][0]], L))  #输出对应的特征参数值，和对应的loss值
        return points, losses

# 获取数据
train_data, test_data = load_data()
x = train_data[:, :-1]   #第一维全取，第二维除最后一个都取
y = train_data[:, -1:]   #第一维全取，第二维取最后一个
# 创建网络
net = Network(13)
num_iterations=2000     #设置迭代次数是2000
# 启动训练
points, losses = net.train(x, y, iterations=num_iterations, eta=0.01)

# 画出损失函数的变化趋势
plot_x = np.arange(num_iterations)   #设置x轴是迭代次数
plot_y = np.array(losses)            #设置loss是y轴
plt.plot(plot_x, plot_y)             #将参数传入，绘图
plt.show()                           #将图展示
```

输出的图片如下

![image-20211216212029845](飞桨教程学习笔记.assets/image-20211216212029845.png)

##### 训练扩展到全部参数

代码反而更加简洁

```python
class Network(object):                                #这里的代码是是选取所有特征参数
    def __init__(self, num_of_weights):               #初始参数值
        # 随机产生w的初始值
        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):                             #向前传播，计算预测值
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):                             #计算损失值
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):                        #计算梯度值
        z = self.forward(x)
        gradient_w = (z-y)*x
        gradient_w = np.mean(gradient_w, axis=0)
        gradient_w = gradient_w[:, np.newaxis]
        gradient_b = (z - y)
        gradient_b = np.mean(gradient_b)        
        return gradient_w, gradient_b
    
    def update(self, gradient_w, gradient_b, eta = 0.01):   #更新梯度值
        self.w = self.w - eta * gradient_w
        self.b = self.b - eta * gradient_b
        
    def train(self, x, y, iterations=100, eta=0.01):       #开始训练
        losses = []                                        #存放损失值
        for i in range(iterations):                        #设置迭代次数
            z = self.forward(x)                            #计算预期值
            L = self.loss(z, y)                            #计算损失值
            gradient_w, gradient_b = self.gradient(x, y)   #计算梯度下降值
            self.update(gradient_w, gradient_b, eta)       #进行更新
            losses.append(L)                               #记录损失
            if (i+1) % 10 == 0:                            #每10次一次输出损失值
                print('iter {}, loss {}'.format(i, L))
        return losses

# 获取数据
train_data, test_data = load_data()
x = train_data[:, :-1]
y = train_data[:, -1:]
# 创建网络
net = Network(13)
num_iterations=1000
# 启动训练
losses = net.train(x,y, iterations=num_iterations, eta=0.01)

# 画出损失函数的变化趋势
plot_x = np.arange(num_iterations)
plot_y = np.array(losses)
plt.plot(plot_x, plot_y)
plt.show()
```

##### 随机梯度下降法

**（Stochastic Gradient Descent）**

![image-20211216212232559](飞桨教程学习笔记.assets/image-20211216212232559.png)

```python
# 获取数据
train_data, test_data = load_data()
train_data.shape
```

train_data中一共包含404条数据，如果batch_size=10，即取前0-9号样本作为第一个mini-batch，命名train_data1。

```python
train_data1 = train_data[0:10]
train_data1.shape
```

使用train_data1的数据（0-9号样本）计算梯度并更新网络参数。

```python
net = Network(13)
x = train_data1[:, :-1]
y = train_data1[:, -1:]
loss = net.train(x, y, iterations=1, eta=0.01)
loss
```

下面取10-19号

```python
train_data2 = train_data[10:20]
x = train_data2[:, :-1]
y = train_data2[:, -1:]
loss = net.train(x, y, iterations=1, eta=0.01)
loss
```

![image-20211216212614268](飞桨教程学习笔记.assets/image-20211216212614268.png)

```python
batch_size = 10                #设置mini_batch大小
n = len(train_data)            #获取数据的长度
mini_batches = [train_data[k:k+batch_size] for k in range(0, n, batch_size)]  #或许mini_batch range从0到n，步长为10
print('total number of mini_batches is ', len(mini_batches))                  #输出batch共有几个
print('first mini_batch shape ', mini_batches[0].shape)                       #第一个mini_batch的形状
print('last mini_batch shape ', mini_batches[-1].shape)                       #最后一个mini_batch的形状
```

![image-20211216212728709](飞桨教程学习笔记.assets/image-20211216212728709.png)

```python
# 新建一个array
a = np.array([1,2,3,4,5,6,7,8,9,10,11,12])
print('before shuffle', a)
np.random.shuffle(a)                 #介绍该函数的作用
print('after shuffle', a)
```

多次运行上面的代码，可以发现每次执行shuffle函数后的数字顺序均不同。 上面举的是一个1维数组乱序的案例，我们再观察下2维数组乱序后的效果。

```python
# 新建一个array
a = np.array([1,2,3,4,5,6,7,8,9,10,11,12])
a = a.reshape([6, 2])
print('before shuffle\n', a)                   #继续说明这个函数的作用
np.random.shuffle(a)
print('after shuffle\n', a)
```

观察运行结果可发现，数组的元素在第0维被随机打乱，但第1维的顺序保持不变。例如数字2仍然紧挨在数字1的后面，数字8仍然紧挨在数字7的后面，而第二维的[3, 4]并不排在[1, 2]的后面。将这部分实现SGD算法的代码集成到Network类中的`train`函数中，最终的完整代码如下。

```python
# 获取数据
train_data, test_data = load_data()

# 打乱样本顺序
np.random.shuffle(train_data)

# 将train_data分成多个mini_batch
batch_size = 10
n = len(train_data)
mini_batches = [train_data[k:k+batch_size] for k in range(0, n, batch_size)]

# 创建网络
net = Network(13)

# 依次使用每个mini_batch的数据
for mini_batch in mini_batches:
    x = mini_batch[:, :-1]
    y = mini_batch[:, -1:]
    loss = net.train(x, y, iterations=1)
```

![image-20211216212917363](飞桨教程学习笔记.assets/image-20211216212917363.png)



**<span id="3.1">最终代码形态</span>**

```python
import numpy as np

class Network(object):
    def __init__(self, num_of_weights):             #初始随机化特征参数
        # 随机产生w的初始值
        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子
        #np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):                           #向前传播
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):                           #计算损失值
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):                      #计算梯度值
        z = self.forward(x)
        N = x.shape[0]
        gradient_w = 1. / N * np.sum((z-y) * x, axis=0)
        gradient_w = gradient_w[:, np.newaxis]
        gradient_b = 1. / N * np.sum(z-y)
        return gradient_w, gradient_b
    
    def update(self, gradient_w, gradient_b, eta = 0.01):      #更新梯度值
        self.w = self.w - eta * gradient_w
        self.b = self.b - eta * gradient_b
            
                
    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):       #封装train函数
        n = len(training_data)                                                 #获取train数据
        losses = []                                                            #存放损失值
        for epoch_id in range(num_epochs):                                     #走epochs，每一次epochs为一次完整的走完数据集
            # 在每轮迭代开始之前，将训练数据的顺序随机打乱
            # 然后再按每次取batch_size条数据的方式取出
            np.random.shuffle(training_data)                                   #将数据打乱
            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据
            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]  #将数据划分mini_batch
            for iter_id, mini_batch in enumerate(mini_batches):                #enumerate 将可遍历的序列加上索引标注
                #print(self.w.shape)
                #print(self.b)
                x = mini_batch[:, :-1]                                         #获取特征值
                y = mini_batch[:, -1:]                                         #获取真实值
                a = self.forward(x)                                            #获取预测值
                loss = self.loss(a, y)                                         #计算损失值
                gradient_w, gradient_b = self.gradient(x, y)                   #计算梯度值
                self.update(gradient_w, gradient_b, eta)                       #更新梯度值
                losses.append(loss)                                            #添加损失值
                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.               #输出epoch值 小的编号值，损失值
                                 format(epoch_id, iter_id, loss))
        
        return losses                                                          #返回所有的损失值

# 获取数据
train_data, test_data = load_data()

# 创建网络
net = Network(13)
# 启动训练
losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.1)

# 画出损失函数的变化趋势
plot_x = np.arange(len(losses))                                              #将迭代次数为x
plot_y = np.array(losses)                                                    #损失值为y轴
plt.plot(plot_x, plot_y)                                                     #进行绘图
plt.show()                                                                   #展示绘图
```

输出图片如下

![image-20211216213033482](飞桨教程学习笔记.assets/image-20211216213033482.png)

![image-20211216213050632](飞桨教程学习笔记.assets/image-20211216213050632.png)

### 总结

本节我们详细介绍了如何使用Numpy实现梯度下降算法，构建并训练了一个简单的线性模型实现波士顿房价预测，可以总结出，使用神经网络建模房价预测有三个要点：

- 构建网络，初始化参数w和b，定义预测和损失函数的计算方法。
- 随机选择初始点，建立梯度的计算方法和参数更新方式。
- 从总的数据集中抽取部分数据作为一个mini_batch，计算梯度并更新参数，不断迭代直到损失函数几乎不再下降。



## 4. 飞浆开源深度学习平台介绍



## 5. 使用飞浆重写房价预测模型



## 6. numpy 介绍