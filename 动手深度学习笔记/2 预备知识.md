# 2 预备知识

## 2.1 数据操作

### 2.1.1 入门

### 2.1.2 运算符

### 2.1.3 广播机制

### 2.1.4 索引和切片

### 2.1.5 节省内存

使用下面两种方法，进行张量的运算，可以节省内存（内存执行原地操作）

```
X[:] =X + Y 
X += Y
```

### 2.1.6 转换为其他python对象

### 2.1.7 小结

 深度学习存储和操作数据主要接口是张量（n维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。

### 2.1.8 练习

1. 运行本节中的代码。将本节中的条件语句`X == Y`更改为`X < Y`或`X > Y`，然后看看你可以得到什么样的张量。
2. 用其他形状（例如三维张量）替换广播机制中按元素操作的两个张量。结果是否与预期相同？



## 2.2 数据预处理

pandas可以与张量兼容，本章节将简要介绍使用pandas预处理原始数据，并将原始数据转换为张量格式的步骤。

### 2.2.1 读取数据集

### 2.2.2 处理缺失值

删除法：直接忽略缺失值

**插值法**：用一个代替值弥补缺失值

通过位置索引`iloc`，我们将`data`分成`inputs`和`outputs`，其中前者为`data`的前两列，而后者为`data`的最后一列。对于`inputs`中缺少的数值，我们用同一列的均值替换“NaN”项。

```python
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean())
```

[**对于`inputs`中的类别值或离散值，我们将“NaN”视为一个类别。**]由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，`pandas`可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。缺少巷子类型的行会将 “Alley_Pave” 和 “Alley_nan”分别设置为0和1。

```python
inputs = pd.get_dummies(inputs, dummy_na=True)
```

### 2.2.3 转换为张量格式

### 2.2.4 小结

-  `pandas`软件包是Python中常用的数据分析工具中，`pandas`可以与张量兼容。
-  用`pandas`处理缺失的数据时，我们可根据情况选择用插值法和删除法。

### 2.2.5 练习

1. 删除缺失值最多的列。

2. 将预处理后的数据集转换为张量格式。

```
inputs = inputs.drop(columns=inputs.isna().sum(axis=0).idxmax())
x = torch.tensor(inputs.values)
```



## 2.3 线性代数

### 2.3.1 标量

### 2.3.2 向量

大量文献认为，**列向量**向量的**默认方向**。

#### 长度、维度和形状

向量的长度通常称为向量的维度（dimension）

### 2.3.3 矩阵

矩阵是有用的数据结构：它允许我们组织具有不同模式的数据。

### 2.3.4 张量

### 2.3.5 张量算法的基本性质

### 2.3.6 降维

#### 非降维求和

如果我们想沿[**某个轴计算`A`元素的累积总和**]，比如`axis=0`（按行计算），我们可以调用`cumsum`函数。此函数不会沿任何轴降低输入张量的维度。

```python
A.cumsum(axis=0)
```

### 2.3.7 点积（Dot Product）

### 2.3.8 矩阵-向量积

### 2.3.9 矩阵-矩阵乘法

### 2.3.10 范数

线性代数中最有用的一些运算符是**范数**（norm）。非正式地说，一个向量的**范数**告诉我们一个向量有多大。这里考虑的**大小**（size）概念不涉及维度，而是分量的大小。

在线性代数中，向量的范数是将向量映射到标量的函数$f$。



你可能会注意到，范数听起来很像距离的度量。如果你还记得欧几里得距离和毕达哥拉斯定理，那么非负性的概念和三角不等式可能会给你一些启发。事实上，欧几里得距离是一个$L_2$范数：假设$n$维向量$\mathbf{x}$中的元素是$x_1,\ldots,x_n$，其[**$L_2$*范数*是向量元素平方和的平方根：**]
$$
\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}
$$
其中，在$L_2$范数中常常省略下标$2$，也就是说$\|\mathbf{x}\|$等同于$\|\mathbf{x}\|_2$。



在深度学习中，我们更经常地使用$L_2$范数的平方。你还会经常遇到[**$L_1$范数，它表示为向量元素的绝对值之和：**]
$$
\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|
$$
与$L_2$范数相比，$L_1$范数受异常值的影响较小。为了计算$L_1$范数，我们将绝对值函数和按元素求和组合起来。




$L_2$范数和$L_1$范数都是更一般的$L_p$范数的特例：
$$
\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}
$$
类似于向量的$L_2$范数，[**矩阵**]$\mathbf{X} \in \mathbb{R}^{m \times n}$(**的*Frobenius范数*（Frobenius norm）是矩阵元素平方和的平方根：**)

$$
\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}
$$


Frobenius范数满足向量范数的所有性质，它就像是矩阵形向量的$L_2$范数。
调用以下函数将计算矩阵的Frobenius范数。




#### 范数和目标

在深度学习中，我们经常试图解决优化问题：**最大化**分配给观测数据的概率;**最小化**预测和真实观测之间的距离。用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。**目标**，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为**范数**。

### 2.3.11 关于线性代数的更多信息

### 2.3.12 小结

- 标量、向量、矩阵和张量是线性代数中的基本数学对象。
- 向量泛化自标量，矩阵泛化自向量。
- 标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。
- 一个张量可以通过`sum`和`mean`沿指定的轴降低维度。
- 两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同。
- 在深度学习中，我们经常使用范数，如$L_1$范数、$L_2$范数和Frobenius范数。
- 我们可以对标量、向量、矩阵和张量执行各种操作。



### 2.3.14 练习

1. 证明一个矩阵$\mathbf{A}$的转置的转置是$\mathbf{A}$，即$(\mathbf{A}^\top)^\top = \mathbf{A}$。
2. 给出两个矩阵$\mathbf{A}$和$\mathbf{B}$，证明“它们转置的和”等于“它们和的转置”，即$\mathbf{A}^\top + \mathbf{B}^\top = (\mathbf{A} + \mathbf{B})^\top$。
3. 给定任意方阵$\mathbf{A}$，$\mathbf{A} + \mathbf{A}^\top$总是对称的吗?为什么?
4. 我们在本节中定义了形状$(2,3,4)$的张量`X`。`len(X)`的输出结果是什么？
5. 对于任意形状的张量`X`,`len(X)`是否总是对应于`X`特定轴的长度?这个轴是什么?
6. 运行`A/A.sum(axis=1)`，看看会发生什么。你能分析原因吗？
7. 考虑一个具有形状$(2,3,4)$的张量，在轴0、1、2上的求和输出是什么形状?
8. 为`linalg.norm`函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?

<img src="https://discuss.d2l.ai/uploads/default/original/2X/d/d4ecced3611b6686fb8ddaeb31d143ccfe488fd2.jpeg" alt="d4ecced3611b6686fb8ddaeb31d143ccfe488fd2.jpeg (1152×3332)" style="zoom:80%;" />

## 2.4 微积分

将拟合模型的任务分解为**两个关键问题**：

- **优化**（optimization）：用模型拟合观测数据的过程
- **泛化**（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。



### 2.4.1 导数和微分

%matplotlib inline就是模仿命令行来访问magic函数的在Python中独有的形式。



```python
if legend is None:
    legend = []

# set_figsize(figsize)
display.set_matplotlib_formats('svg')
d2l.plt.rcParams['figure.figsize'] = figsize # matplotlibrc动态配置

# 如果未传入axes的值则获取画布当前的axes（本例会默认创建）
axes = axes if axes else d2l.plt.gca()

# 如果 `X` 有一个轴，输出True
def has_one_axis(X):
    return (hasattr(X, "ndim") and X.ndim == 1 or # X有ndim属性且其值为1
            isinstance(X, list) and not hasattr(X[0], "__len__")) # X为列表且其首个元素没有"__len__"属性

if has_one_axis(X):
    # 如果是一个轴，将其列表化
    X = [X]

if Y is None:
    # 如果没有传入Y参数，X为包含X长度个空列表的大列表，Y为X
    X, Y = [[]] * len(X), X

elif has_one_axis(Y):
    # 如果 Y 只有一个轴，将其列表化
    Y = [Y]

if len(X) != len(Y):
    # 如果X的长度和Y的长度不一样，X改为Y长度数个X
    X = X * len(Y)

axes.cla() # 清除axes

for x, y, fmt in zip(X, Y, fmts):
    if len(x): # 如果X的元素x不为空
        axes.plot(x, y, fmt)
    else:
        axes.plot(y, fmt)
```

### 2.4.2 偏导数

### 2.4.3 梯度

### 2.4.4 链式法则

### 2.4.5 小结

- 微分和积分是微积分的两个分支，前者可以应用于深度学习中的优化问题。
- 导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。
- 梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。
- 链式法则使我们能够微分复合函数。



### 2.4.6 练习

1. 绘制函数$y = f(x) = x^3 - \frac{1}{x}$和其在$x = 1$处切线的图像。
2. 求函数$f(\mathbf{x}) = 3x_1^2 + 5e^{x_2}$的梯度。
3. 函数$f(\mathbf{x}) = \|\mathbf{x}\|_2$的梯度是什么？
4. 你可以写出函数$u = f(x, y, z)$，其中$x = x(a, b)$，$y = y(a, b)$，$z = z(a, b)$的链式法则吗?

```python
#第一题
plot(x, [x**3-1/x, 4*x-4], ‘x’, ‘f(x)’, legend=[‘f(x)’, ‘Tangent line (x=1)’]
```

<img src="https://discuss.d2l.ai/uploads/default/original/2X/2/2874e9507ba661f92a0636d3fa7abe019e6ffaf3.jpeg" alt="img" style="zoom: 67%;" />



## 2.5 自动微分

深度学习框架通过自动计算导数，即**自动微分**（automatic differentiation）来加快求导。实际中，根据我们设计的模型，系统会构建一个**计算图**（computational graph），来跟踪计算是哪些数据通过哪些操作组合起来产生输出。自动微分使系统能够随后反向传播梯度。这里，**反向传播**（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。



### 2.5.1 一个简单的例子

### 2.5.2 非标量变量的反向传播

### 2.5.3 分离计算

### 2.5.4 Python控制流的梯度计算

使用自动微分的一个好处是：

[**即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度**]。

### 2.5.5 小结

深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上。然后我们记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。

### 2.5.6 练习

1. 为什么计算二阶导数比一阶导数的开销要更大？
2. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。
3. 在控制流的例子中，我们计算`d`关于`a`的导数，如果我们将变量`a`更改为随机向量或矩阵，会发生什么？
4. 重新设计一个求控制流梯度的例子，运行并分析结果。
5. 使$f(x)=\sin(x)$，绘制$f(x)$和$\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\cos(x)$。

<img src="https://discuss.d2l.ai/uploads/default/original/2X/6/68eff5c8416b2a0b72d220facb6de3598571e956.jpeg" alt="img" style="zoom: 67%;" />

## 2.6 概率！

**(概率计算不够熟练，需要好好去了解一下)**

概率是一种灵活的语言，用于说明我们的确定程度，并且它可以有效地应用于广泛的领域中。

### 2.6.1 基本概率论



#### 概率论公理

**概率**（probability）可以被认为是将集合映射到真实值的函数。在给定的样本空间$\mathcal{S}$中，事件$\mathcal{A}$的概率，表示为$P(\mathcal{A})$，满足以下属性：

- 对于任意事件$\mathcal{A}$，其概率从不会是负数，即$P(\mathcal{A}) \geq 0$；
- 整个样本空间的概率为$1$，即$P(\mathcal{S}) = 1$；
- 对于**互斥**（mutually exclusive）事件（对于所有$i \neq j$都有$\mathcal{A}_i \cap \mathcal{A}_j = \emptyset$）的任意一个可数序列$\mathcal{A}_1, \mathcal{A}_2, \ldots$，序列中任意一个事件发生的概率等于它们各自发生的概率之和，即$P(\bigcup_{i=1}^{\infty} \mathcal{A}_i) = \sum_{i=1}^{\infty} P(\mathcal{A}_i)$。



#### 随机变量

### 2.6.2 处理多个随机变量

#### 联合概率

第一个被称为**联合概率**（joint probability）$P(A=a,B=b)$。给定任意值$a$和$b$，联合概率可以回答：$A=a$和$B=b$同时满足的概率是多少？ 

#### 条件概率

联合概率的不等式带给我们一个有趣的比率：$0 \leq \frac{P(A=a, B=b)}{P(A=a)} \leq 1$。我们称这个比率为**条件概率**（conditional probability），并用$P(B=b \mid A=a)$表示它：它是$B=b$的概率，前提是$A=a$已发生。

#### 贝叶斯定理

使用条件概率的定义，我们可以得出统计学中最有用的方程之一：**Bayes定理**（Bayes' theorem）。根据**乘法法则**（multiplication rule ）可得到$P(A, B) = P(B \mid A) P(A)$。根据对称性，可得到$P(A, B) = P(A \mid B) P(B)$。假设$P(B)>0$，求解其中一个条件变量，我们得到：
$$
P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}
$$
请注意，这里我们使用紧凑的表示法：其中$P(A, B)$是一个**联合分布**（joint distribution），$P(A \mid B)$是一个**条件分布**（conditional distribution）。这种分布可以在给定值$A = a, B=b$上进行求值。

#### 边际化

为了能进行事件概率求和，我们需要**求和法则**（sum rule），即$B$的概率相当于计算$A$的所有可能选择，并将所有选择的联合概率聚合在一起：
$$
P(B) = \sum_{A} P(A, B)
$$
这也称为**边际化**（marginalization）。边际化结果的概率或分布称为**边际概率**（marginal probability）或**边际分布**（marginal distribution）。

#### 独立性

#### 应用



### 2.6.3 期望和方差



### 2.6.4 小结

- 我们可以从概率分布中采样。
- 我们可以使用联合分布、条件分布、Bayes定理、边缘化和独立性假设来分析多个随机变量。
- 期望和方差为概率分布的关键特征的概括提供了实用的度量形式。



### 2.6.5 练习

1. 进行$m=500$组实验，每组抽取$n=10$个样本。改变$m$和$n$，观察和分析实验结果。
2. 给定两个概率为$P(\mathcal{A})$和$P(\mathcal{B})$的事件，计算$P(\mathcal{A} \cup \mathcal{B})$和$P(\mathcal{A} \cap \mathcal{B})$的上限和下限。（提示：使用[友元图](https://en.wikipedia.org/wiki/Venn_diagram)来展示这些情况。)
3. 假设我们有一系列随机变量，例如$A$、$B$和$C$，其中$B$只依赖于$A$，而$C$只依赖于$B$，你能简化联合概率$P(A, B, C)$吗？（提示：这是一个[马尔可夫链](https://en.wikipedia.org/wiki/Markov_chain)。)
4. 在 :numref:`subsec_probability_hiv_app`中，第一个测试更准确。为什么不运行第一个测试两次，而是同时运行第一个和第二个测试?

**第一题**
设定 m = 5000, n = 10，得到结果如下：

<img src="https://img-blog.csdnimg.cn/20210403111402307.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyODkwODAw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

设定 m = 500, n = 100，得到结果如下：

<img src="https://img-blog.csdnimg.cn/20210403111330720.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyODkwODAw,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:50%;" />

对比以上结果可知，第一张图效果更好，更加趋近于 0.167。原因可能是第一张图实验组数为 5000 组，而第二张图只有 500 组，虽然两者总实验次数相同，但是第一张图的实验组数要更多一些。考虑到每一组都可能存在偏差，实验组数越多，实验偏差就会分布的更加平均，效果就会更好。

**第二题**
对于$ P ( A ∪ B ) P(A \cup B)P(A∪B)$，上限是 $P(A) + P(B)$，下限是 $max\{P(A), P(B)\}$。

对于$P ( A ∩ B P(A \cap BP(A∩B)$，上限是$min\{P(A), P(B)\}$，下限是 0。

**第三题**
$P(A, B, C) = P(A) P(B|A) P(C|B)$

**第四题**
原因是如果进行两次第一种测试，两次测试没有条件独立性，不能使用上面的公式进行计算，而且两次测试结果大概率相同，效果并不好。

## 2.7 查阅资料

### 2.7.1  查找模块中的所有函数和类

### 2.7.2 查找特定函数和类的用法

### 2.7.3 小结

### 2.7.4 练习