# 7 现代卷积神经网络

## 7.1 深度卷积神经网络（AlexNet）

### 7.1.1 学习表征

深度卷积神经网络的突破出现在2012年。突破可归因于两个关键因素。

#### 缺少的成分：数据

这项被称为ImageNet的挑战赛推动了计算机视觉和机器学习研究的发展，挑战研究人员确定哪些模型能够在更大的数据规模下表现最好。

#### 缺少的成分：硬件

回到2012年的重大突破，当Alex Krizhevsky和Ilya Sutskever实现了可以在GPU硬件上运行的深度卷积神经网络时，一个重大突破出现了。他们意识到卷积神经网络中的计算瓶颈：卷积和矩阵乘法，都是可以在硬件上并行化的操作。

### 7.1.2 AlexNet

<img src="https://zh-v2.d2l.ai/_images/alexnet.svg" alt="../_images/alexnet.svg" style="zoom: 67%;" />

<center>从LeNet（左）到AlexNet（右）</center>

AlexNet和LeNet的设计理念非常相似，但也存在显著差异。 首先，AlexNet比相对较小的LeNet5要深得多。 AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。 其次，AlexNet使用ReLU而不是sigmoid作为其激活函数。

#### 模型设计

#### 激活函数

此外，AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。 一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。 另一方面，当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易。

#### 容量控制和预处理

AlexNet通过暂退法控制全连接层的模型复杂度，而LeNet只使用了权重衰减。 为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。 这使得模型更健壮，更大的样本量有效地减少了过拟合。



### 7.1.3 读取数据集

### 7.1.4 训练AlexNet

### 7.1.5 小结

* AlexNet的架构与LeNet相似，但使用了更多的卷积层和更多的参数来拟合大规模的ImageNet数据集。
* 今天，AlexNet已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步。
* 尽管AlexNet的代码只比LeNet多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具。
* Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤。

### 7.1.6 练习

1. 试着增加迭代轮数。对比LeNet的结果有什么不同？为什么？
1. AlexNet对于Fashion-MNIST数据集来说可能太复杂了。
   1. 尝试简化模型以加快训练速度，同时确保准确性不会显著下降。
   1. 设计一个更好的模型，可以直接在$28 \times 28$图像上工作。
1. 修改批量大小，并观察模型精度和GPU显存变化。
1. 分析了AlexNet的计算性能。
   1. 在AlexNet中主要是哪部分占用显存？
   1. 在AlexNet中主要是哪部分需要更多的计算？
   1. 计算结果时显存带宽如何？
1. 将dropout和ReLU应用于LeNet-5，效果有提升吗？再试试预处理会怎么样？



## 7.2 使用块的网络

### 7.2.1 VGG块

### 7.2.2 VGG网络

<img src="https://zh-v2.d2l.ai/_images/vgg.svg" alt="../_images/vgg.svg" style="zoom:67%;" />

<center>从AlexNet到VGG，它们本质上都是块设计。</center>



### 7.2.3 训练模型

### 7.2.4 小结

* VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。
* 块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。
* 在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即$3 \times 3$）比较浅层且宽的卷积更有效。

### 7.2.5 练习

1. 打印层的尺寸时，我们只看到8个结果，而不是11个结果。剩余的3层信息去哪了？
1. 与AlexNet相比，VGG的计算要慢得多，而且它还需要更多的显存。分析出现这种情况的原因。
1. 尝试将Fashion-MNIST数据集图像的高度和宽度从224改为96。这对实验有什么影响？
1. 请参考VGG论文 :cite:`Simonyan.Zisserman.2014`中的表1构建其他常见模型，如VGG-16或VGG-19。



## 7.3 网络中的网络（NiN）

LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。***网络中的网络***（*NiN*）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机 :cite:`Lin.Chen.Yan.2013`

### 7.3.1 NiN块

NiN块以一个普通卷积层开始，后面是两个$1 \times 1$的卷积层。这两个$1 \times 1$卷积层充当带有ReLU激活函数的逐像素全连接层。第一层的卷积窗口形状通常由用户设置。随后的卷积窗口形状固定为$1 \times 1$。

<img src="https://zh-v2.d2l.ai/_images/nin.svg" alt="../_images/nin.svg" style="zoom:67%;" />

<center>对比 VGG 和 NiN 及它们的块之间主要架构差异。</center>

### 7.3.2 NiN模型

### 7.3.3 训练模型 

### 7.3.4 小结

* NiN使用由一个卷积层和多个$1\times 1$卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。
* NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。
* 移除全连接层可减少过拟合，同时显著减少NiN的参数。
* NiN的设计影响了许多后续卷积神经网络的设计。

### 7.3.5 练习

1. 调整NiN的超参数，以提高分类准确性。
1. 为什么NiN块中有两个$1\times 1$卷积层？删除其中一个，然后观察和分析实验现象。
1. 计算NiN的资源使用情况。
   1. 参数的数量是多少？
   1. 计算量是多少？
   1. 训练期间需要多少显存？
   1. 预测期间需要多少显存？
1. 一次性直接将$384 \times 5 \times 5$的表示缩减为$10 \times 5 \times 5$的表示，会存在哪些问题？



## 7.4 含并行连接的网络（GoogLeNet）

 GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。 这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题。

### 7.4.1 Inception块

<img src="https://zh-v2.d2l.ai/_images/inception.svg" alt="../_images/inception.svg" style="zoom: 80%;" />

<center>Inception块的架构</center>

那么为什么GoogLeNet这个网络如此有效呢？ 首先我们考虑一下滤波器（filter）的组合，它们可以用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。 同时，我们可以为不同的滤波器分配不同数量的参数。

### 7.4.2 GoogLeNet模型

<img src="https://zh-v2.d2l.ai/_images/inception-full.svg" alt="../_images/inception-full.svg" style="zoom:67%;" />

<center>GoogLeNet架构</center>

### 7.4.3 训练模型

### 7.4.4 小结

* Inception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用$1×1$卷积层减少每像素级别上的通道维数从而降低模型复杂度。
* GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。
* GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。

### 7.4.5 练习

1. GoogLeNet有一些后续版本。尝试实现并运行它们，然后观察实验结果。这些后续版本包括：
   * 添加批量规范化层 :cite:`Ioffe.Szegedy.2015`（batch normalization），在 :numref:`sec_batch_norm`中将介绍。
   * 对Inception模块进行调整 :cite:`Szegedy.Vanhoucke.Ioffe.ea.2016`。
   * 使用标签平滑（label smoothing）进行模型正则化 :cite:`Szegedy.Vanhoucke.Ioffe.ea.2016`。
   * 加入残差连接 :cite:`Szegedy.Ioffe.Vanhoucke.ea.2017`。（ :numref:`sec_resnet`将介绍）。
1. 使用GoogLeNet的最小图像大小是多少？
1. 将AlexNet、VGG和NiN的模型参数大小与GoogLeNet进行比较。后两个网络架构是如何显著减少模型参数大小的？



## 7.5 批量规范化

批量规范化可持续加速深层网络的收敛速度。

### 7.5.1 训练深层网络

### 7.5.2 批量规范化层

批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。 接下来，我们应用比例系数和比例偏移。 正是由于这个基于*批量*统计的*标准化*，才有了*批量规范化*的名称。

#### 全连接层

#### 卷积层

#### 预测过程中的批量规范化

### 7.5.3 从零实现

### 7.5.4 使用批量规范化层的LeNet

### 7.5.5 简明实现

### 7.5.6 争议

### 7.5.7 小结

* 在模型训练过程中，批量规范化利用小批量的均值和标准差，不断调整神经网络的中间输出，使整个神经网络各层的中间输出值更加稳定。
* 批量规范化在全连接层和卷积层的使用略有不同。
* 批量规范化层和暂退层一样，在训练模式和预测模式下计算不同。
* 批量规范化有许多有益的副作用，主要是正则化。另一方面，”减少内部协变量偏移“的原始动机似乎不是一个有效的解释。

### 7.5.8 练习

1. 在使用批量规范化之前，我们是否可以从全连接层或卷积层中删除偏置参数？为什么？
1. 比较LeNet在使用和不使用批量规范化情况下的学习率。
   1. 绘制训练和测试准确度的提高。
   1. 你的学习率有多高？
1. 我们是否需要在每个层中进行批量规范化？尝试一下？
1. 你可以通过批量规范化来替换暂退法吗？行为会如何改变？
1. 确定参数`beta`和`gamma`，并观察和分析结果。
1. 查看高级API中有关`BatchNorm`的在线文档，以查看其他批量规范化的应用。
1. 研究思路：想想你可以应用的其他“规范化”转换？你可以应用概率积分变换吗？全秩协方差估计可以么？



## 7.6 残差网络（RestNet）

### 7.6.1 函数类

残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。 于是，*残差块*（residual blocks）便诞生了，这个设计对如何建立深层神经网络产生了深远的影响。

### 7.6.2 残差块

<img src="https://zh-v2.d2l.ai/_images/residual-block.svg" alt="../_images/residual-block.svg" style="zoom:80%;" />

<center>一个正常块（左图）和一个残差块（右图）。</center>

### 7.6.3 ResNet模型

### 7.6.4 训练模型

### 7.6.5 小结

* 学习嵌套函数（nested function）是训练神经网络的理想情况。在深层神经网络中，学习另一层作为恒等映射（identity function）较容易（尽管这是一个极端情况）。
* 残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。
* 利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。
* 残差网络（ResNet）对随后的深层神经网络设计产生了深远影响。

### 7.6.6 练习

1.  :numref:`fig_inception`中的Inception块与残差块之间的主要区别是什么？在删除了Inception块中的一些路径之后，它们是如何相互关联的？
1.  参考ResNet论文 :cite:`He.Zhang.Ren.ea.2016`中的表1，以实现不同的变体。
1.  对于更深层次的网络，ResNet引入了“bottleneck”架构来降低模型复杂性。请你试着去实现它。
1.  在ResNet的后续版本中，作者将“卷积层、批量规范化层和激活层”架构更改为“批量规范化层、激活层和卷积层”架构。请你做这个改进。详见 :cite:`He.Zhang.Ren.ea.2016*1`中的图1。
1.  为什么即使函数类是嵌套的，我们仍然要限制增加函数的复杂性呢？



## 7.7 稠密链接网络（DenseNet）

### 7.7.1 从ResNet到DenseNet

<img src="https://zh-v2.d2l.ai/_images/densenet-block.svg" alt="../_images/densenet-block.svg" style="zoom:80%;" />

<center>ResNet（左）与 DenseNet（右）在跨层连接上的主要区别：使用相加和使用连结</center>

 DenseNet这个名字由变量之间的“稠密连接”而得来，最后一层与之前的所有层紧密相连。

<img src="https://zh-v2.d2l.ai/_images/densenet.svg" alt="../_images/densenet.svg" style="zoom:80%;" />

<center>稠密连接。</center>



### 7.7.2 稠密块体

### 7.7.3 过渡层

### 7.7.4 DenseNet 模型

### 7.7.5 训练模型 

### 7.7.6 小结

* 在跨层连接上，不同于ResNet中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。
* DenseNet的主要构建模块是稠密块和过渡层。
* 在构建DenseNet时，我们需要通过添加过渡层来控制网络的维数，从而再次减少通道的数量。

### 7.7.7 练习


1. 为什么我们在过渡层使用平均汇聚层而不是最大汇聚层？
1. DenseNet的优点之一是其模型参数比ResNet小。为什么呢？
1. DenseNet一个诟病的问题是内存或显存消耗过多。
   1. 真的是这样吗？可以把输入形状换成$224 \times 224$，来看看实际的显存消耗。
   1. 你能想出另一种方法来减少显存消耗吗？你需要如何改变框架？
1. 实现DenseNet论文 :cite:`Huang.Liu.Van-Der-Maaten.ea.2017`表1所示的不同DenseNet版本。
1. 应用DenseNet的思想设计一个基于多层感知机的模型。将其应用于 :numref:`sec_kaggle_house`中的房价预测任务。

