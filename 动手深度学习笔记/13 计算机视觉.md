# 13 计算机视觉

## 13.1 图像增广

### 13.1.1 常用的图像增广方法

#### 翻转和裁剪

#### 改变颜色

#### 结合多种图像增广方法

### 13.1.2 使用图像增广进行训练

#### 多GPU训练

### 13.1.3 小结

* 图像增广基于现有的训练数据生成随机图像，来提高模型的泛化能力。
* 为了在预测过程中得到确切的结果，我们通常对训练样本只进行图像增广，而在预测过程中不使用带随机操作的图像增广。
* 深度学习框架提供了许多不同的图像增广方法，这些方法可以被同时应用。

### 13.1.4 练习

1. 在不使用图像增广的情况下训练模型：`train_with_data_aug(no_aug, no_aug)`。比较使用和不使用图像增广的训练结果和测试精度。这个对比实验能支持图像增广可以减轻过拟合的论点吗？为什么？
2. 在基于CIFAR-10数据集的模型训练中结合多种不同的图像增广方法。它能提高测试准确性吗？
3. 参阅深度学习框架的在线文档。它还提供了哪些其他的图像增广方法？



## 13.2 微调

### 13.2.1 步骤

### 13.2.2 热狗识别

#### 获取数据集

#### 定义和初始化模型

#### 微调模型

### 13.2.3 小结

* 迁移学习将从源数据集中学到的知识“迁移”到目标数据集，微调是迁移学习的常见技巧。
* 除输出层外，目标模型从源模型中复制所有模型设计及其参数，并根据目标数据集对这些参数进行微调。但是，目标模型的输出层需要从头开始训练。
* 通常，微调参数使用较小的学习率，而从头开始训练输出层可以使用更大的学习率。

### 13.2.4 练习

1. 继续提高`finetune_net`的学习率，模型的准确性如何变化？
2. 在比较实验中进一步调整`finetune_net`和`scratch_net`的超参数。它们的准确性还有不同吗？
3. 将输出层`finetune_net`之前的参数设置为源模型的参数，在训练期间不要更新它们。模型的准确性如何变化？你可以使用以下代码。
4. 事实上，`ImageNet`数据集中有一个“热狗”类别。我们可以通过以下代码获取其输出层中的相应权重参数，但是我们怎样才能利用这个权重参数？



## 13.3 目标检测和边界框

### 13.3.1 边界框

### 13.3.2 小结

* 目标检测不仅可以识别图像中所有感兴趣的物体，还能识别它们的位置，该位置通常由矩形边界框表示。
* 我们可以在两种常用的边界框表示（中间，宽度，高度）和（左上，右下）坐标之间进行转换。

### 13.3.3 练习

1. 找到另一张图像，然后尝试标记包含该对象的边界框。比较标注边界框和标注类别哪个需要更长的时间？
1. 为什么`box_corner_to_center`和`box_center_to_corner`的输入参数的最内层维度总是4？



## 13.4 锚框

### 13.4.1 生成多个锚框

### 13.4.2 交并比（IoU）

### 13.4.3 在训练数据中标注锚框

#### 将真实边界框分配给锚框

#### 标记类别和偏移量

#### 一个例子

### 13.4.4 使用非极大值抑制预测边界框

### 13.4.5 小结

* 我们以图像的每个像素为中心生成不同形状的锚框。
* 交并比（IoU）也被称为杰卡德系数，用于衡量两个边界框的相似性。它是相交面积与相并面积的比率。
* 在训练集中，我们需要给每个锚框两种类型的标签。一个是与锚框中目标检测的类别，另一个是锚框真实相对于边界框的偏移量。
* 在预测期间，我们可以使用非极大值抑制（NMS）来移除类似的预测边界框，从而简化输出。

### 13.4.6 练习

1. 在`multibox_prior`函数中更改`sizes`和`ratios`的值。生成的锚框有什么变化？
1. 构建并可视化两个IoU为0.5的边界框。它们是怎样重叠的？
1. 在 :numref:`subsec_labeling-anchor-boxes`和 :numref:`subsec_predicting-bounding-boxes-nms`中修改变量`anchors`，结果如何变化？
1. 非极大值抑制是一种贪心算法，它通过*移除*来抑制预测的边界框。是否存在一种可能，被移除的一些框实际上是有用的？如何修改这个算法来柔和地抑制？你可以参考Soft-NMS :cite:`Bodla.Singh.Chellappa.ea.2017`。
1. 如果非手动，非最大限度的抑制可以被学习吗？



## 13.5 多尺度目标检测

### 13.5.1 多尺度锚框

### 13.5.2 多尺度检测

### 13.5.3 小结

* 在多个尺度下，我们可以生成不同尺寸的锚框来检测不同尺寸的目标。
* 通过定义特征图的形状，我们可以决定任何图像上均匀采样的锚框的中心。
* 我们使用输入图像在某个感受野区域内的信息，来预测输入图像上与该区域位置相近的锚框类别和偏移量。
* 我们可以通过深入学习，在多个层次上的图像分层表示进行多尺度目标检测。

### 13.5.4 练习

1. 根据我们在 :numref:`sec_alexnet`中的讨论，深度神经网络学习图像特征级别抽象层次，随网络深度的增加而升级。在多尺度目标检测中，不同尺度的特征映射是否对应于不同的抽象层次？为什么？
1. 在 :numref:`subsec_multiscale-anchor-boxes`中的实验里的第一个尺度（`fmap_w=4, fmap_h=4`）下，生成可能重叠的均匀分布的锚框。
1. 给定形状为$1 \times c \times h \times w$的特征图变量，其中$c$、$h$和$w$分别是特征图的通道数、高度和宽度。你怎样才能将这个变量转换为锚框类别和偏移量？输出的形状是什么？



## 13.6 目标检测数据

### 13.6.1 下载数据集

### 13.6.2 读取数据集

### 13.6.3 演示

### 13.6.4 小结

* 我们收集的香蕉检测数据集可用于演示目标检测模型。
* 用于目标检测的数据加载与图像分类的数据加载类似。但是，在目标检测中，标签还包含真实边界框的信息，它不出现在图像分类中。

### 13.6.5 练习

1. 在香蕉检测数据集中演示其他带有真实边界框的图像。它们在边界框和目标方面有什么不同？
1. 假设我们想要将数据增强（例如随机裁剪）应用于目标检测。它与图像分类中的有什么不同？提示：如果裁剪的图像只包含物体的一小部分会怎样？



## 13.7 单发多框检测（SSD）

### 13.7.1 模型

#### 类别预测层

#### 边界框预测框

#### 连结多尺度的预测

#### 高和宽减半块

#### 基本网络块

#### 完整的模型

### 13.7.2 训练模型

#### 定义损失函数和评价函数

#### 训练模型

### 13.7.3 预测目标

### 13.7.4 小结

* 单发多框检测是一种多尺度目标检测模型。基于基础网络块和各个多尺度特征块，单发多框检测生成不同数量和不同大小的锚框，并通过预测这些锚框的类别和偏移量检测不同大小的目标。
* 在训练单发多框检测模型时，损失函数是根据锚框的类别和偏移量的预测及标注值计算得出的。

### 13.7.5 练习

1. 你能通过改进损失函数来改进单发多框检测吗？

2. 由于篇幅限制，我们在本节中省略了单发多框检测模型的一些实现细节。你能否从以下几个方面进一步改进模型：
   1. 当目标比图像小得多时，模型可以将输入图像调大。
   1. 通常会存在大量的负锚框。为了使类别分布更加平衡，我们可以将负锚框的高和宽减半。
   1. 在损失函数中，给类别损失和偏移损失设置不同比重的超参数。
   1. 使用其他方法评估目标检测模型，例如单发多框检测论文 :cite:`Liu.Anguelov.Erhan.ea.2016`中的方法。



## 13.8 区域卷积神经网络（R-CNN）系列

### 13.8.1 R-CNN

### 13.8.2 Fast R-CNN

### 13.8.3 Faster R-CNN

### 13.8.4 Mask R-CNN

### 13.8.5 小结


* R-CNN对图像选取若干提议区域，使用卷积神经网络对每个提议区域执行前向传播以抽取其特征，然后再用这些特征来预测提议区域的类别和边界框。
* Fast R-CNN对R-CNN的一个主要改进：只对整个图像做卷积神经网络的前向传播。它还引入了兴趣区域汇聚层，从而为具有不同形状的兴趣区域抽取相同形状的特征。
* Faster R-CNN将Fast R-CNN中使用的选择性搜索替换为参与训练的区域提议网络，这样后者可以在减少提议区域数量的情况下仍保证目标检测的精度。
* Mask R-CNN在Faster R-CNN的基础上引入了一个全卷积网络，从而借助目标的像素级位置进一步提升目标检测的精度。

### 13.8.6 练习

1. 我们能否将目标检测视为回归问题（例如预测边界框和类别的概率）？你可以参考YOLO模型 :cite:`Redmon.Divvala.Girshick.ea.2016`的设计。
1. 将单发多框检测与本节介绍的方法进行比较。他们的主要区别是什么？你可以参考 :cite:`Zhao.Zheng.Xu.ea.2019`中的图2。



## 13.9 语义分割和数据集

### 13.9.1 图像分割和实例分割

### 13.9.2 Pascal VOC2012语义分割数据集

#### 预处理数据

#### 自定义语义分割数据集类

#### 读取数据集

#### 整合所有组件

### 13.9.3 小结

* 语义分割通过将图像划分为属于不同语义类别的区域，来识别并理解图像中像素级别的内容。
* 语义分割的一个重要的数据集叫做Pascal VOC2012。
* 由于语义分割的输入图像和标签在像素上一一对应，输入图像会被随机裁剪为固定尺寸而不是缩放。

### 13.9.4 练习

1. 如何在自动驾驶和医疗图像诊断中应用语义分割？还能想到其他领域的应用吗？
1. 回想一下 :numref:`sec_image_augmentation`中对数据增强的描述。图像分类中使用的哪种图像增强方法是难以用于语义分割的？





## 13.10 转置卷积

### 13.10.1 基本操作

### 13.10.2 填充、步幅和多通道

### 13.10.3 与矩阵变化的联系

### 13.10.4 小结

* 与通过卷积核减少输入元素的常规卷积相反，转置卷积通过卷积核广播输入元素，从而产生形状大于输入的输出。
* 如果我们将$\mathsf{X}$输入卷积层$f$来获得输出$\mathsf{Y}=f(\mathsf{X})$并创造一个与$f$有相同的超参数、但输出通道数是$\mathsf{X}$中通道数的转置卷积层$g$，那么$g(Y)$的形状将与$\mathsf{X}$相同。
* 我们可以使用矩阵乘法来实现卷积。转置卷积层能够交换卷积层的正向传播函数和反向传播函数。

### 13.10.5 练习

1. 在 :numref:`subsec-connection-to-mat-transposition`中，卷积输入`X`和转置的卷积输出`Z`具有相同的形状。他们的数值也相同吗？为什么？
1. 使用矩阵乘法来实现卷积是否有效率？为什么？



## 13.11 全卷积网络

### 13.11.1 构造模型

### 13.11.2 初始化转置卷积层

### 13.11.3 读取数据集

### 13.11.4 训练集

### 13.11.5 预测

### 13.11.6 小结

* 全卷积网络先使用卷积神经网络抽取图像特征，然后通过$1\times 1$卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。
* 在全卷积网络中，我们可以将转置卷积层初始化为双线性插值的上采样。

### 13.11.7 练习

1. 如果将转置卷积层改用Xavier随机初始化，结果有什么变化？
1. 调节超参数，能进一步提升模型的精度吗？
1. 预测测试图像中所有像素的类别。
1. 最初的全卷积网络的论文中 :cite:`Long.Shelhamer.Darrell.2015`还使用了某些卷积神经网络中间层的输出。试着实现这个想法。



## 13.12 风格迁移

### 13.12.1 方法

### 13.12.2 阅读内容和风格图像

### 13.12.3 预处理和后处理

### 13.12.4 抽取图像特征

### 13.12.5 定义损失函数

#### 内容损失

#### 风格损失

#### 全变分损失

#### 损失函数

### 13.12.6 初始化合成图像

### 13.12.7 训练模型

### 13.12.8 小结

* 风格迁移常用的损失函数由3部分组成：（i）内容损失使合成图像与内容图像在内容特征上接近；（ii）风格损失令合成图像与风格图像在风格特征上接近；（iii）全变分损失则有助于减少合成图像中的噪点。
* 我们可以通过预训练的卷积神经网络来抽取图像的特征，并通过最小化损失函数来不断更新合成图像来作为模型参数。
* 我们使用格拉姆矩阵表达风格层输出的风格。

### 13.12.9 练习

1. 选择不同的内容和风格层，输出有什么变化？
1. 调整损失函数中的权重超参数。输出是否保留更多内容或减少更多噪点？
1. 替换实验中的内容图像和风格图像，你能创作出更有趣的合成图像吗？
1. 我们可以对文本使用风格迁移吗？提示:你可以参阅调查报告 :cite:`Hu.Lee.Aggarwal.ea.2020`。



## 13.13 实战Kaggle比赛：图像分类（CIFAR-10）

### 13.13.1 获取并组织数据集

#### 下载数据集

#### 整理数据集

### 13.13.2 图像增广

### 13.13.3 读取数据集

### 13.13.4 定义模型

### 13.13.5 定义训练函数

### 13.13.6 训练和验证模型

### 13.13.7 在Kaggle上对测试集进行分类并提交结果

### 13.13.8 小结

* 将包含原始图像文件的数据集组织为所需格式后，我们可以读取它们。
* 我们可以在图像分类竞赛中使用卷积神经网络和图像增广。

### 13.13.9 练习

1. 在这场Kaggle竞赛中使用完整的CIFAR-10数据集。将超参数设为`batch_size = 128`，`num_epochs = 100`，`lr = 0.1`，`lr_period = 50`，`lr_decay = 0.1`。看看你在这场比赛中能达到什么准确度和排名。或者你能进一步改进吗？
1. 不使用图像增广时，你能获得怎样的准确度？



## 13.14 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）

### 13.14.1 获取和整理数据集

#### 下载数据集

#### 整理数据集

### 13.14.2 图像增广

### 13.14.3 读取数据集

### 13.14.4 微调预训练模型

### 13.14.5 定义训练函数

### 13.14.6 训练和验证模型

### 13.14.7 对测试集分类并在Kaggle提交结果

### 13.14.8 小结

* ImageNet数据集中的图像比CIFAR-10图像尺寸大，我们可能会修改不同数据集上任务的图像增广操作。
* 要对ImageNet数据集的子集进行分类，我们可以利用完整ImageNet数据集上的预训练模型来提取特征并仅训练小型自定义输出网络，这将减少计算时间和节省内存空间。

### 13.14.9 练习

1. 试试使用完整Kaggle比赛数据集，增加`batch_size`（批量大小）和`num_epochs`（迭代轮数），或者设计其它超参数为`lr = 0.01`，`lr_period = 10`，和`lr_decay = 0.1`时，你能取得什么结果？
1. 如果你使用更深的预训练模型，会得到更好的结果吗？如何调整超参数？能进一步改善结果吗？

