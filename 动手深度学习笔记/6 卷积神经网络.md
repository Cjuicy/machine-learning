# 6 卷积神经网络

## 6.1 从全连接层到卷积

### 6.1.1 不变性

卷积神经网络正是将*空间不变性*（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。

1. *平移不变性*（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
1. *局部性*（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。

### 6.1.2 多层感知机的限制

#### 平移不变性

#### 局部性

### 6.1.3 卷积

### 6.1.4 “沃尔多在哪里”回顾

#### 通道

### 6.1.5 小结

- 图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。
- 局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。
- 在图像处理中，卷积层通常比全连接层需要更少的参数，但依旧获得高效用的模型。
- 卷积神经网络（CNN）是一类特殊的神经网络，它可以包含多个卷积层。
- 多个输入和输出通道使模型在每个空间位置可以获取图像的多方面特征。

### 6.1.6 练习

1. 假设卷积层 :eqref:`eq_conv-layer`覆盖的局部区域$\Delta = 0$。在这种情况下，证明卷积内核为每组通道独立地实现一个全连接层。
1. 为什么平移不变性可能也不是好主意呢？
1. 当从图像边界像素获取隐藏表示时，我们需要思考哪些问题？
1. 描述一个类似的音频卷积层的架构。
1. 卷积层也适合于文本数据吗？为什么？
1. 证明在 :eqref:`eq_2d-conv-discrete`中，$f * g = g * f$。



## 6.2 图像卷积

### 6.2.1 互相关联运算

### 6.2.2 卷积层

### 6.2.3 图像中目标的边缘检测

### 6.2.4 学习卷积核

### 6.2.5 互相关和卷积

### 6.2.6 特征映射和感受野

### 6.2.7 小结

* 二维卷积层的核心计算是二维互相关运算。最简单的形式是，对二维输入数据和卷积核执行互相关操作，然后添加一个偏置。
* 我们可以设计一个卷积核来检测图像的边缘。
* 我们可以从数据中学习卷积核的参数。
* 学习卷积核时，无论用严格卷积运算或互相关运算，卷积层的输出不会受太大影响。
* 当需要检测输入特征中更广区域时，我们可以构建一个更深的卷积网络。

### 6.2.8 练习

1. 构建一个具有对角线边缘的图像`X`。
   1. 如果将本节中举例的卷积核`K`应用于`X`，会发生什么情况？
   1. 如果转置`X`会发生什么？
   1. 如果转置`K`会发生什么？
1. 在我们创建的`Conv2D`自动求导时，有什么错误消息？
1. 如何通过改变输入张量和卷积核张量，将互相关运算表示为矩阵乘法？
1. 手工设计一些卷积核：
   1. 二阶导数的核的形式是什么？
   1. 积分的核的形式是什么？
   1. 得到$d$次导数的最小核的大小是多少？



## 6.3 填充和步幅

### 6.3.1 填充

### 6.3.2 步幅

### 6.3.3 小结

* 填充可以增加输出的高度和宽度。这常用来使输出与输入具有相同的高和宽。
* 步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的$1/n$（$n$是一个大于$1$的整数）。
* 填充和步幅可用于有效地调整数据的维度。

### 6.3.4 练习


1. 对于本节中的最后一个示例，计算其输出形状，以查看它是否与实验结果一致。
1. 在本节中的实验中，试一试其他填充和步幅组合。
1. 对于音频信号，步幅$2$说明什么？
1. 步幅大于$1$的计算优势是什么？



## 6.4 多输入多输出通道

### 6.4.1 多输入通道

### 6.4.2 多输出通道

### 6.4.3 1 × 1 卷积层

### 6.4.4 小结

* 多输入多输出通道可以用来扩展卷积层的模型。
* 当以每像素为基础应用时，$1\times 1$卷积层相当于全连接层。
* $1\times 1$卷积层通常用于调整网络层的通道数量和控制模型复杂性。

### 6.4.5  练习

1. 假设我们有两个卷积核，大小分别为$k_1$和$k_2$（中间没有非线性激活函数）。
   1. 证明运算可以用单次卷积来表示。
   1. 这个等效的单个卷积核的维数是多少呢？
   1. 反之亦然吗？
1. 假设输入为$c_i\times h\times w$，卷积核大小为$c_o\times c_i\times k_h\times k_w$，填充为$(p_h, p_w)$，步幅为$(s_h, s_w)$。
   1. 前向传播的计算成本（乘法和加法）是多少？
   1. 内存占用是多少？
   1. 反向传播的内存占用是多少？
   1. 反向传播的计算成本是多少？
1. 如果我们将输入通道$c_i$和输出通道$c_o$的数量加倍，计算数量会增加多少？如果我们把填充数量翻一番会怎么样？
1. 如果卷积核的高度和宽度是$k_h=k_w=1$，前向传播的计算复杂度是多少？
1. 本节最后一个示例中的变量`Y1`和`Y2`是否完全相同？为什么？
1. 当卷积窗口不是$1\times 1$时，如何使用矩阵乘法实现卷积？



## 6.5 汇聚层（池化层）

### 6.5.1 最大汇聚层和平均汇聚层

### 6.5.2 填充和步幅

### 6.5.3 多个通道

### 6.5.4 小结

* 对于给定输入元素，最大汇聚层会输出该窗口内的最大值，平均汇聚层会输出该窗口内的平均值。
* 汇聚层的主要优点之一是减轻卷积层对位置的过度敏感。
* 我们可以指定汇聚层的填充和步幅。
* 使用最大汇聚层以及大于1的步幅，可减少空间维度（如高度和宽度）。
* 汇聚层的输出通道数与输入通道数相同。

### 6.5.5 练习

1. 你能将平均汇聚层作为卷积层的特殊情况实现吗？
1. 你能将最大汇聚层作为卷积层的特殊情况实现吗？
1. 假设汇聚层的输入大小为$c\times h\times w$，则汇聚窗口的形状为$p_h\times p_w$，填充为$(p_h, p_w)$，步幅为$(s_h, s_w)$。这个汇聚层的计算成本是多少？
1. 为什么最大汇聚层和平均汇聚层的工作方式不同？
1. 我们是否需要最小汇聚层？可以用已知函数替换它吗？
1. 除了平均汇聚层和最大汇聚层，是否有其它函数可以考虑（提示：回想一下`softmax`）？为什么它不流行？



## 6.6 卷积神经网络（LeNet）

### 6.6.1 LeNet

<img src="https://zh-v2.d2l.ai/_images/lenet.svg" alt="../_images/lenet.svg" style="zoom:67%;" />

<center>LeNet中的数据流。输入是手写数字，输出为10种可能结果的概率。</center>



### 6.6.2 模型训练

### 6.6.3 小结

* 卷积神经网络（CNN）是一类使用卷积层的网络。
* 在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层。
* 为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。
* 在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。
* LeNet是最早发布的卷积神经网络之一。

### 6.6.4 练习

1. 将平均汇聚层替换为最大汇聚层，会发生什么？
1. 尝试构建一个基于LeNet的更复杂的网络，以提高其准确性。
   1. 调整卷积窗口大小。
   1. 调整输出通道的数量。
   1. 调整激活函数（如ReLU）。
   1. 调整卷积层的数量。
   1. 调整全连接层的数量。
   1. 调整学习率和其他训练细节（例如，初始化和轮数）。
1. 在MNIST数据集上尝试以上改进的网络。
1. 显示不同输入（例如毛衣和外套）时，LeNet第一层和第二层的激活值。

