# 第二章 模型评估与选择

## 2.1经验误差与过拟合

**训练误差（training error）/或经验误差（empirical error）**

学习器在训练集上的误差



**泛化误差（generalization error）**

在新样本上的误差



**过拟合（overfitting）**

学习器学的太好，把训练样本自身的一些特点当做了所有潜在样本都会具有的一般性质，导致泛化性能下降



**欠拟合（underfitting）**

指对训练样本的一般性质尚未学好



**模型选择（model selection）**

当使用不同的参数配置，也会产生不同的模型，我们应该选择哪一个学习算法、使用哪一种参数配置？这就是机器学习中的“模型选择”问题。



## 2.2评估方法

**测试集（testing set）**

用于测试学习器对新样本的判别能力。



**测试误差（testing error）**

作为泛化误差的近似



既要训练又要测试，有什么方法呢？



### 2.2.1留出法

**留出法（hold-out）**

直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S,另一个作为测试集T。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。



**注意：**

训练/测试集的划分要尽可能保持数据分布一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响。



**从采样（sampling）的角度来看待数据集的划分过程**

保留类别比例的采样方式通常称为“分层采样”（stratified sampling）

不同的划分将导致不同的训练/测试集



**保真性（fidelity）**



**常见做法**

将大约2/3~4/5的样本用于训练，剩余的样本用于测试



### 2.2.2交叉验证法

交叉验证法（cross validation）先将数据集D划分为k个大小相似的互斥子集，每个子集D_i都尽可能保持数据分布的一致性，即从D中通过分层采样得到。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集；这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这个k个测试结果的均值。

通常把交叉验证法称为“k折交叉验证”（k-fold cross validation）

k最常用的取值是10，此时称为10折交叉验证

k折交叉验证通常要随机使用不同的划分重复p次



**留一法（Leave-One-Out——LOO）**

交叉验证的特例，与初始数据集相比只少了一个样本。留一法的评估结往往被认为比较准确。

**缺陷：**

在数据集较大时，训练m个模型的计算开销可能是难以忍受的。



### 2.2.3自助法

**自助法（bootstrapping）**

减少训练样本规模不同造成的影响，同时还能比较高效地进行实验估计

直接以自助采样法（bootstrap sampling）为基础，给定包含m个样本的数据集D，我们对它进行采样产生数据集D’，每次随机从D总挑选一个样本的数据集D‘，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被采到，这个过程重复执行m次后，我们就得到了包含m个样本的数据集D’，这就是自助采样的结果。



**包外估计（out-of-bag estimate）**



**优点：**

自助法在数据集较小，难以有效划分训练/训练测试集时很有用处。

**缺点：**

自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差



### 2.2.4调参与最终模型

大多数学习算法都有写参数（parameter）需要设定，参数配置不同，学习模型的性能往往有显著的差别。除了要对使用学习算法进行选择，还需要对算法那参数进行设定，这就是通常所说的“参数调节”或简称“调参”（parameter tuning）。



**另外注意：**
我们通常把学得模型在实际使用中遇到的数据称为测试数据，为了加以区分，模型评估与选择中用于评估测试的数据集常称为“验证集”（validation set）



## 2.3性能度量

**性能度量（performance measure）**

衡量模型泛化能力的评价标准



**均方误差（mean squared error）**

回归任务最常用的性能度量

<img src="周志华机器学习笔记.assets/image-20211116105448301.png" alt="image-20211116105448301" style="zoom: 80%;" />

更一般的，对数据分布D个概率密度函数p(·)，均方误差的描述

<img src="周志华机器学习笔记.assets/image-20211116105604621.png" alt="image-20211116105604621" style="zoom: 80%;" />



### 2.3.1错误率与精度

**错误率**

分类错误的样本数占样本总数的比例

<img src="周志华机器学习笔记.assets/image-20211116105826664.png" alt="image-20211116105826664" style="zoom:80%;" />

更一般的，对数据分布D个概率密度函数p(·)，的错误率

<img src="周志华机器学习笔记.assets/image-20211116105944762.png" alt="image-20211116105944762" style="zoom:80%;" />







**精度**

分类正确的样本数占样本总数的比例

<img src="周志华机器学习笔记.assets/image-20211116105841104.png" alt="image-20211116105841104" style="zoom:80%;" />

更一般的，对数据分布D个概率密度函数p(·)，的精度

<img src="周志华机器学习笔记.assets/image-20211116105957227.png" alt="image-20211116105957227" style="zoom:80%;" />





### 2.3.2查准率、查全率与F1

查准率（precision）与查全率（recall）用于满足更加细致的需求，也是一种性能度量



**TP**：真正例（true positive）

**FP**：假正例（false positive）

**TN**：真反例（true negative）

**FN**：假反例（false negative）

TP+FP+TN+FN = 样例总数

| 真实情况 | 预测结果     | 预测结果     |
| -------- | ------------ | ------------ |
| 真实情况 | 正例         | 反例         |
| 正例     | TP（真正例） | FN（假反例） |
| 反例     | FP（假正例） | TN（真反例） |



查准率P和查全率R分别定义为

<img src="周志华机器学习笔记.assets/image-20211116111249346.png" alt="image-20211116111249346" style="zoom:80%;" />

查准率和查全率是一对矛盾的度量



**P-R图**可以直观地显示出学习器在样本总体上的查全率、查准率

若一个学习器的P-R曲线被另一个学习器的曲线完全“**包住**”，则可断言后者的性能优于前者

如果发生了交叉，但是我们还是想要比较高下，这时一个比较合理的判断是比较P-R曲线下**面积的大小**



**平衡点（Break-Even Point——BEP）**

它是查**准率 = 查全率**时的取值



**F1度量（是BEP的优化）**

基于查准率与查全率的调和平均（harmonic mean）

<img src="周志华机器学习笔记.assets/image-20211116112106098.png" alt="image-20211116112106098" style="zoom:80%;" />



**F_β（F1度量的一般形式）**

用于对查准率与查全率不同的需求

能够让我们表达出队查准率/查全率的不同偏好

<img src="周志华机器学习笔记.assets/image-20211116112456541.png" alt="image-20211116112456541" style="zoom:80%;" />



**再多个混淆矩阵上综合考察查准率和查全率**

先计算查准率和查全率，然后再计算平均值

宏查准率（macro-P）

宏查全率（macro-R）

宏F1（macro-F1）

<img src="周志华机器学习笔记.assets/image-20211116112803352.png" alt="image-20211116112803352" style="zoom:80%;" />



**先对混淆矩阵（误差矩阵）进行平均，得到TP、FP、TN、FN的平均值，在计算出查准率，查全率**

微查准率（micro-P）

微查全率（micro-R）

微F1（micro-F1）

<img src="周志华机器学习笔记.assets/image-20211116113143980.png" alt="image-20211116113143980" style="zoom:80%;" />

<img src="周志华机器学习笔记.assets/image-20211116113201462.png" alt="image-20211116113201462" style="zoom:80%;" />



### 2.3.3ROC与AUC

很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值（threshold）进行比较，若大于阈值则分为正类，否则为反类。

重视“查准率”，选择排序中靠前的位置进行截断

重视“查全率”，选择排序中靠后的位置进行截断

为了综合考虑学习器在不同任务下的“泛化性能”的好坏，或者说“一般情况下”泛化性能的好坏。ROC曲线则思从这个角度出发来研究学习器泛化性能的有力工具

**ROC——受试者工作特征（Receiver Operation Characterristic）曲线**

与P-R曲线使用查准率。查全率为纵、横轴不同，ROC曲线的纵轴是**“真正例率**”（True Positive Rate,简称TPR），横轴是“**假正例率**”（False Positive Rate，简称FPR）

<img src="周志华机器学习笔记.assets/image-20211117103949689.png" alt="image-20211117103949689" style="zoom:80%;" />

**AUC（Area under ROC Curve）**

若一个学习器的ROC曲线被另一个学习器的曲线完全“包住”，则可断言后者的性能优于前者。若两个学习器的ROC曲线发生交叉，那么合理的判据是比较ROC曲线下的面积，即AUC（Area under ROC Curve）

<img src="周志华机器学习笔记.assets/image-20211117104259250.png" alt="image-20211117104259250" style="zoom:80%;" />



结合图表，这就是一个梯形公式

下图是另一种计算公式，给定m+个正例，m-个反例，令D+和D-分别表示正、反例集合，则排序“Loss”损失定义为

<img src="周志华机器学习笔记.assets/image-20211117104340936.png" alt="image-20211117104340936" style="zoom:80%;" />

即考虑每一对正、反例，若正例的预测值小于反例，则记一个“罚分”，若相等，则记0.5个“罚分”、容易看出l_rank对应的是ROC曲线之上的面积。

<img src="周志华机器学习笔记.assets/image-20211117104647112.png" alt="image-20211117104647112" style="zoom:80%;" />

为了帮助理解提供两个视频连接：

**master学堂**

https://www.bilibili.com/video/BV1H54y1D7or?share_source=copy_web

**小萌五分钟**

https://www.bilibili.com/video/BV1wz4y197LU?share_source=copy_web





### 2.3.4代价敏感错误率与代价曲线

为了权衡不同类型错误所造成的不同损失，可为错误赋予**“非均等代价”（unequal cost）**

我们根据任务的领域知识设定一个**“代价矩阵”（cost matrix）**

其中cost_ij 表示将第i类样本预测为第j类样本的代价。

损失程度相差越大，cost01与cost02值的差别越大

<img src="周志华机器学习笔记.assets/image-20211117143942728.png" alt="image-20211117143942728" style="zoom:80%;" />

在非均等代价下，我们所希望的不再是简单地最小化错误次数，而是希望最小化**“总体代价”（total cost）**

将上图第0类作为正类，第1类作为反类，D+与D-分别代表样例集D的正例子集，和反例子集，则**“代价敏感”（cost-sensitive）**错误率为

<img src="周志华机器学习笔记.assets/image-20211117144311888.png" alt="image-20211117144311888" style="zoom:80%;" />

若令cost_ij中的i，j取值不限于0、1则可以定义出多分类任务的代价敏感性能度量



ROC曲线不能直接反映出学习器的期望总体代价，而**“代价曲线”（cost curve）**则可达到该目的。代价曲线图的横轴是取值为[0,1]的正例概率代价

<img src="周志华机器学习笔记.assets/image-20211117144615243.png" alt="image-20211117144615243" style="zoom:80%;" />

其中p是样例为正例的概率；纵轴是取值为[0,1]的归一化代价

<img src="周志华机器学习笔记.assets/image-20211117144718516.png" alt="image-20211117144718516" style="zoom:80%;" />

（“规范化”normalization 是将不同变化范围的值映射到相同的固定范围中，常见的是[0,1]，此时亦称“归一化”）

FPR是式子定义的假正例率，FNR=1-TPR是假反例率

<img src="周志华机器学习笔记.assets/image-20211117151455293.png" alt="image-20211117151455293" style="zoom:67%;" />

## 2.4比较检验

机器学习的性能比较，要比上面的更加复杂。

1. 我们希望比较的是泛化性能，然而通过实验评估方法我们获得的是测试集上的性能，两者的对比结果可能未必相同。
2. 测试集上的性能与测试集本身的选择有很大的关系，且不论使用不同大小的测试集会得到不同的结果，即便使用相同大小的测试集，若包含的测试样例不同，测试结果也会不同。
3. 很多机器学习算法本身有一定的随机性，即便用相同的参数设置在同一个测试集上多次运行，其结果也会有所不同。



统计假设检验（hypothesis test）为我们进行学习器性能比较提供了重要依据。



本节默认以错误率为性能度量，用ε表示



### 2.4.1假设检验

 (看不懂)

二项检验（binomial test）

置信度（confidence）

t检验（t-test）

双边（two-tailed）



### 2.4.2交叉验证t检验

使用了5x2交叉验证，能大概看懂

成对t检验（paired t-tests）



### 2.4.3McNemar检验

列联表（contingency table）



### 2.4.4Friendman检验与Nemeny后续检验





## 2.5偏差与方差











